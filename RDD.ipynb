{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD\n",
    "# 3 methods\n",
    "# 1st: pass an existing object to SparkContext's parallelize method\n",
    "# 2nd: load data from an external hard drive (HDFS) or from Amazon s3 bucket or lines from a text\n",
    "# 3rd: from an existing RDD\n",
    "\n",
    "# 1st method: parallelized collection \n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize('Hello World')\n",
    "type(helloRDD)\n",
    "\n",
    "# create RDD using the external datasets, textFile() method\n",
    "fileRDD = sc.textFile('README.md')\n",
    "type(fileRDD)\n",
    "\n",
    "\n",
    "# LOAD DATA INTO RDDs\n",
    "# understanding how Spark deals with partitions allows us to control parallelism\n",
    "# create an RDD using SparkContext's parallelize method with 6 partitions\n",
    "numRDD = sc.parallelize(range(10), minPartitions=6)\n",
    "\n",
    "# or we can use this method\n",
    "numRDD = sc.textFile('README.md', minPartitions=6)\n",
    "\n",
    "print('The number of partitions in numRDD is', numRDD.getNumPartitions())\n",
    "\n",
    "# RDD operations in PySpark\n",
    "# operations = transformation + actions\n",
    "# transformation creates RDD, action computes on RDDs\n",
    "# transformations follow Lazy evaluation, which enables RDDs to be fault tolerant\n",
    "\n",
    "# map() transformation\n",
    "# create RDD using SparkContext's parallelize method\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "# apply map function to each element\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "\n",
    "# filter transformation returns a new RDD with elements that pass the condition\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "\n",
    "# flatMap transformation returns many values for each element in the original RDD\n",
    "RDD = sc.parallelize(['Hello world', 'How are you'])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# union transformation returns the union of one RDD with another RDD\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda x: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are operations applied to RDD and return a value\n",
    "# basic RDD actions: collect(), take(N), first(), count()\n",
    "RDD_map.collect()\n",
    "RDD_map.take(2)\n",
    "RDD_map.first()\n",
    "RDD_flatmap.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with RDD key/value pairs\n",
    "# pair RDDs: special data structure\n",
    "# pair RDDs: key = identifier, value = data\n",
    "# 2 ways to create a pair RDD: from a list of key-value tuple or from a regular RDD\n",
    "# get the data into the key/value form\n",
    "\n",
    "# create a pair RDD from a list of key-value tuple\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# create a pair RDD from regular RDDs\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "# some transformations for pairRDDs: reduceByKey(), groupByKey(), sortByKey(), join()\n",
    "\n",
    "# practice reduceByKey() transformation\n",
    "regularRDD = sc.parallelize([\"Messi\", 23], [\"Ronaldo\", 34]\n",
    "                            [\"Neymar\", 22], [\"Messi\", 24])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y: x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "\n",
    "# sorting of data: sortByKey() transformation\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "\n",
    "# group values with the same key: groupByKey()\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "    \n",
    "# join() transformation to connect two pair RDDs based on their key\n",
    "RDD1 = sc.parallelize([(\"Messi\", 24), (\"Ronaldo\", 32)], (\"Neymar\", 24))\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 40), (\"Neymar\", 120), (\"Messi\", 50)])\n",
    "RDD1.join(RDD2).collect()\n",
    "\n",
    "# Practice\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# ADVANCED RDD actions\n",
    "# reduce() action - avoid collect() because of the size\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "# saveAsTextFile() - each partition is saved separately as a file inside a directory\n",
    "RDD.saveAsTextFile(\"FileName\")\n",
    "# coalesce() - save RDD as a single text file\n",
    "RDD.coalesce(1).saveAsTextFile(\"FileName\")\n",
    "\n",
    "# RDD actions: countByKey(), collectAsMap()\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for k, v in rdd.countByKey().items():\n",
    "    print(k, v)\n",
    "    \n",
    "# collectAsMap(): returns k-v pairs as dict\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "\n",
    "# Practice\n",
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "\n",
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n",
    "\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# PySpark DATAFRAME\n",
    "# Spark high level API for working with structured data\n",
    "# PySpark SQL is a Spark library for structured data\n",
    "# PySpark SQL provides the structure of data and the computation being performed\n",
    "# PySpark SQL provides a programming abstraction called DataFrames\n",
    "# SparkSession does for DataFrames what the SparkContext does for RDDs\n",
    "# SparkSession creates DataFrames, registers DataFrames as tables, executes SQL over tables, cache tables\n",
    "# DataFrames can be created by spark in two ways: from existing RDDs - createDataFrame(), from various sources using\n",
    "# SparkSession's read method\n",
    "# Schema is a structure of data in DataFrame, it helps Spark optimize queries on the data more efficiently\n",
    "\n",
    "# CREATE a DataFrame from RDD\n",
    "iphones_RDD = sc.parallelize([(\"XR\", 2018, 5.65, 2.79, 6.24),\n",
    "                              (\"Xs\", 2018, 5.94, 2.98, 6.84)])\n",
    "names = [\"Model\", \"Year\", \"Height\", \"Width\", \"Weight\"]\n",
    "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\n",
    "type(iphones_df)\n",
    "\n",
    "# CREATE a DataFrame from a csv file\n",
    "df_csv = spark.read.csv(\"filename.csv\", header=True, inferSchema=True)\n",
    "# CREATE a DataFrame from a json file\n",
    "df_csv = spark.read.json(\"filename.json\", header=True, inferSchema=True)\n",
    "\n",
    "# Practice\n",
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
