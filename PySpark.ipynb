{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD\n",
    "# 3 methods\n",
    "# 1st: pass an existing object to SparkContext's parallelize method\n",
    "# 2nd: load data from an external hard drive (HDFS) or from Amazon s3 bucket or lines from a text\n",
    "# 3rd: from an existing RDD\n",
    "\n",
    "# 1st method: parallelized collection \n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize('Hello World')\n",
    "type(helloRDD)\n",
    "\n",
    "# create RDD using the external datasets, textFile() method\n",
    "fileRDD = sc.textFile('README.md')\n",
    "type(fileRDD)\n",
    "\n",
    "\n",
    "# LOAD DATA INTO RDDs\n",
    "# understanding how Spark deals with partitions allows us to control parallelism\n",
    "# create an RDD using SparkContext's parallelize method with 6 partitions\n",
    "numRDD = sc.parallelize(range(10), minPartitions=6)\n",
    "\n",
    "# or we can use this method\n",
    "numRDD = sc.textFile('README.md', minPartitions=6)\n",
    "\n",
    "print('The number of partitions in numRDD is', numRDD.getNumPartitions())\n",
    "\n",
    "# RDD operations in PySpark\n",
    "# operations = transformation + actions\n",
    "# transformation creates RDD, action computes on RDDs\n",
    "# transformations follow Lazy evaluation, which enables RDDs to be fault tolerant\n",
    "\n",
    "# map() transformation\n",
    "# create RDD using SparkContext's parallelize method\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "# apply map function to each element\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "\n",
    "# filter transformation returns a new RDD with elements that pass the condition\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "\n",
    "# flatMap transformation returns many values for each element in the original RDD\n",
    "RDD = sc.parallelize(['Hello world', 'How are you'])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# union transformation returns the union of one RDD with another RDD\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda x: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are operations applied to RDD and return a value\n",
    "# basic RDD actions: collect(), take(N), first(), count()\n",
    "RDD_map.collect()\n",
    "RDD_map.take(2)\n",
    "RDD_map.first()\n",
    "RDD_flatmap.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with RDD key/value pairs\n",
    "# pair RDDs: special data structure\n",
    "# pair RDDs: key = identifier, value = data\n",
    "# 2 ways to create a pair RDD: from a list of key-value tuple or from a regular RDD\n",
    "# get the data into the key/value form\n",
    "\n",
    "# create a pair RDD from a list of key-value tuple\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# create a pair RDD from regular RDDs\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "# some transformations for pairRDDs: reduceByKey(), groupByKey(), sortByKey(), join()\n",
    "\n",
    "# practice reduceByKey() transformation\n",
    "regularRDD = sc.parallelize([\"Messi\", 23], [\"Ronaldo\", 34]\n",
    "                            [\"Neymar\", 22], [\"Messi\", 24])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y: x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "\n",
    "# sorting of data: sortByKey() transformation\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "\n",
    "# group values with the same key: groupByKey()\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "    \n",
    "# join() transformation to connect two pair RDDs based on their key\n",
    "RDD1 = sc.parallelize([(\"Messi\", 24), (\"Ronaldo\", 32)], (\"Neymar\", 24))\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 40), (\"Neymar\", 120), (\"Messi\", 50)])\n",
    "RDD1.join(RDD2).collect()\n",
    "\n",
    "# Practice\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# ADVANCED RDD actions\n",
    "# reduce() action - avoid collect() because of the size\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "# saveAsTextFile() - each partition is saved separately as a file inside a directory\n",
    "RDD.saveAsTextFile(\"FileName\")\n",
    "# coalesce() - save RDD as a single text file\n",
    "RDD.coalesce(1).saveAsTextFile(\"FileName\")\n",
    "\n",
    "# RDD actions: countByKey(), collectAsMap()\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for k, v in rdd.countByKey().items():\n",
    "    print(k, v)\n",
    "    \n",
    "# collectAsMap(): returns k-v pairs as dict\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "\n",
    "# Practice\n",
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "\n",
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n",
    "\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# PySpark DATAFRAME\n",
    "# Spark high level API for working with structured data\n",
    "# PySpark SQL is a Spark library for structured data\n",
    "# PySpark SQL provides the structure of data and the computation being performed\n",
    "# PySpark SQL provides a programming abstraction called DataFrames\n",
    "# SparkSession does for DataFrames what the SparkContext does for RDDs\n",
    "# SparkSession creates DataFrames, registers DataFrames as tables, executes SQL over tables, cache tables\n",
    "# DataFrames can be created by spark in two ways: from existing RDDs - createDataFrame(), from various sources using\n",
    "# SparkSession's read method\n",
    "# Schema is a structure of data in DataFrame, it helps Spark optimize queries on the data more efficiently\n",
    "\n",
    "# CREATE a DataFrame from RDD\n",
    "iphones_RDD = sc.parallelize([(\"XR\", 2018, 5.65, 2.79, 6.24),\n",
    "                              (\"Xs\", 2018, 5.94, 2.98, 6.84)])\n",
    "names = [\"Model\", \"Year\", \"Height\", \"Width\", \"Weight\"]\n",
    "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\n",
    "type(iphones_df)\n",
    "\n",
    "# CREATE a DataFrame from a csv file\n",
    "df_csv = spark.read.csv(\"filename.csv\", header=True, inferSchema=True)\n",
    "# CREATE a DataFrame from a json file\n",
    "df_csv = spark.read.json(\"filename.json\", header=True, inferSchema=True)\n",
    "\n",
    "# Practice\n",
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))\n",
    "\n",
    "# INTERACTING WITH PYSPARK DATAFRAMES\n",
    "# DataFrames support transformations and actions\n",
    "# Common DataFrame transformations: select(), filter(), groupby(), orderby(), dropDuplicates(), withColumnRenamed()\n",
    "# Common DataFrame actions: printSchema(), head(), show(), count(), columns(), describe()\n",
    "\n",
    "# select and show operations\n",
    "# pass column names inside select()\n",
    "df_id_age = test.select('Age')\n",
    "# show is an action that prints the first 20 rows by default\n",
    "df_id_age.show(3)\n",
    "# filter() filters out rows based on a condition, pass the column name and the value of what we want to filter\n",
    "new_df_age21 = new_df.filter(new_df.Age > 21)\n",
    "new_df_age21.show(3)\n",
    "\n",
    "# groupby() groups a variable\n",
    "test_df_age_group = test_df.groupby('Age')\n",
    "test_df_age_group.count().show(3)\n",
    "\n",
    "# orderby() sorts the dataframe based on one or more columns\n",
    "test_df_age_group.count().orderBy('Age').show(3)\n",
    "\n",
    "# dropDuplicates() removes the duplicate rows of a DataFrame\n",
    "test_df_no_dup = test_df.select('User ID', 'Gender', 'Age').dropDuplicates()\n",
    "test_df_no_dup.count()\n",
    "\n",
    "# withColumnRenamed() renames a column in the DataFrame\n",
    "test_df_sex = test_df.withColumnRenamed('Gender', 'Sex')\n",
    "\n",
    "# printSchema() prints the types of columns in DataFrames\n",
    "test_df.printSchema()\n",
    "\n",
    "# columns() prints the names of columns in DataFrame\n",
    "test_df.columns()\n",
    "\n",
    "# describe() summarizes the statistics of numerical columns in DataFrame\n",
    "test_df.describe().show()\n",
    "\n",
    "# PRACTICE\n",
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
    "\n",
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))\n",
    "\n",
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))\n",
    "\n",
    "\n",
    "# INTERACT WITH PySpark SQL using SQL Query\n",
    "# PySpark SQL - high level API built on top of Spark Core for structured data\n",
    "# PySpark can interact with SparkSQL through DataFrame API and SQL Queries\n",
    "# SparkSession sql() executes SQL query\n",
    "# sql() takes a SQL statement as an argument and returns the results as DataFrame\n",
    "# SQL Queries cannot be run directly against DataFrames\n",
    "# To issue SQL Queries against an existing DataFrame, use createOrReplaceTempView() to build a temporary table\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT field1, field 2 FROM table1\")\n",
    "df2.collect()\n",
    "\n",
    "# SQL query to extract data\n",
    "test_df.createOrReplaceTempView(\"test_table\")\n",
    "query = '''SELECT Product_ID FROM test_table'''\n",
    "test_product_df = spark.sql(query)\n",
    "test_product_df.show(5)\n",
    "\n",
    "# PRACTICE\n",
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)\n",
    "\n",
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))\n",
    "\n",
    "# DATA VISUALIZATION with PySpark using DataFrames\n",
    "# 3 methods to plot graphs using PySpark DataFrames\n",
    "# 1st: pyspark_dist_explore\n",
    "# 2nd: toPandas()\n",
    "# 3rd: HandySpark\n",
    "\n",
    "# pyspark_dist_explore: hist(), distplot(), pandas_histogram()\n",
    "# draw a histogram\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "test_df_age = test_df.select('Age')\n",
    "hist(test_df_age, bins=20, color=\"red\")\n",
    "\n",
    "# convert PySpark DataFrames into Pandas DataFrames\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "test_df_sample_pandas = test_df_sample.toPandas()\n",
    "test_df_sample_pandas.hist('Age')\n",
    "\n",
    "# Pandas DataFrames vs PySpark DataFrames\n",
    "# Pandas = 1 server memory, PySpark = multiple servers\n",
    "# Pandas = mutable, PySpark = immutable\n",
    "# Pandas APIs support more operations\n",
    "\n",
    "# HandySpark method\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "hdf = test_df.toHandy()\n",
    "hdf.cols['Age'].hist()\n",
    "\n",
    "# PRACTICE\n",
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()\n",
    "\n",
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))\n",
    "\n",
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT * FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()\n",
    "\n",
    "\n",
    "# PySpark MLlib - a built-in library for scalable machine learning\n",
    "\n",
    "# Machine Learning - a scientific discipline that explores the construction and study of algorithms that can learn from data\n",
    "# PySpark MLlib provides tools such as algorithms (collaborative filtering, classification, clustering)\n",
    "# Scikit-Learn is a very popular and easy to use Python library for machine learning and data mining\n",
    "# Scikit-Learn only works for small datasets and on a single machine\n",
    "# PySpark MLlib algorithms are designed for parallel processing on a cluster, support Java, Scala and R\n",
    "# MLlib provides high level API to build ML pipelines\n",
    "# Including Classification (Binary and Multiclass) and Regression: Linear SVMs, logistic regression, decision trees, random forests,\n",
    "# gradient-boosted trees, naive Bayes, linear least squares, Lasso, ridge regression, isotonic regression\n",
    "# Collaborative filtering: alternating least squares (ALS)\n",
    "# Clustering: K-means, Gaussian mixture, Bisecting K-means and Streaming K-means\n",
    "# Three Key Areas (Three C's) of machine learning in PySpark MLlib: Collaborative Filtering, Classification, Clustering\n",
    "\n",
    "# PySpark MLlib imports\n",
    "pyspark.mllib.recommendation\n",
    "from pyspark.mllib import ALS\n",
    "\n",
    "pyspark.mllib.classification\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "pyspark.mllib.clustering\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "# Collaborative Filtering\n",
    "# is a method of making automatic predictions about the interests of the users by collecting preferences from other users\n",
    "# 2 ways: \n",
    "# user-user collaborative filtering - finds users that are similar to the targeted user \n",
    "# item-item collaborative filtering - finds items that are similar to items interested by the targeted user\n",
    "\n",
    "# Rating class is a wrapper around tuple (user, product, rating)\n",
    "# in pyspark.mllib.recommendation submodule\n",
    "# useful for parsing RDDs, creating a tuple of user, product and rating\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "r = Rating(user=1, product=2, rating=5.0)\n",
    "\n",
    "# splitting the data with randomSplit()\n",
    "# splitting the data to evaluate the predictive modeling\n",
    "\n",
    "# PRACTICE\n",
    "# Load the data into RDD\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)\n",
    "\n",
    "# Prepare ratings data\n",
    "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification is a supervised machine learning algorithm for sorting the input data into different categories\n",
    "# Binary vs Multiclass\n",
    "# Linear SVMs, logistic regression, decision trees, random forests, gradient-boosted trees, naive Bayes, etc\n",
    "# Logistic Regression to predict a binary response, the output must be 0 or 1. If the probability is > 50%, output is 1.\n",
    "# Working with Vectors and LabelledPoints\n",
    "# Vectors: dense and sparse\n",
    "# Dense Vectors: store all of their entries in an array of floating point numbers\n",
    "# Sparse Vectors: store only nonzero values and their indices\n",
    "denseVec = Vectors.dense([1.0, 2.0, 3.0])\n",
    "sparseVec = Vectors.sparse(4, {1: 1.0, 3: 5.5})\n",
    "# LabelledPoint is a wrapper for input features and predicted value\n",
    "positive = LabelledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "negative = LabelledPoint(0.0, [2.0, 1.0, 1.0])\n",
    "# HashingTF() computes a term frequency vector of a given size, used to map feature value to indices in the feature vector\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "sentence = \"hello hello world\"\n",
    "words = sentence.split()\n",
    "tf = HashingTF(10000)\n",
    "tf.transform(words)\n",
    "\n",
    "# Logistic Regression using LogisticRegressionWithLBFGS\n",
    "# The minimum requirement for LogisticRegressionWithLBFGS is an RDD of LabelledPoint\n",
    "data = [\n",
    "        LabelledPoint(0.0, [0.0, 1.0]),\n",
    "        LabelledPoint(1.0, [1.0, 0.0])\n",
    "]\n",
    "RDD = sc.parallelize(data)\n",
    "lrm = LogisticRegressionLBFGS.train(RDD)\n",
    "lrm.predict([1.0, 0.0])\n",
    "lrm.predict([0.0, 1.0])\n",
    "\n",
    "# PRACTICE\n",
    "\n",
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(file_path_spam)\n",
    "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())\n",
    "\n",
    "# Create a HashingTf instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.join(non_spam_samples)\n",
    "\n",
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering is an unsupervised method to group unlabeled data together\n",
    "# Grouping similar objects into clusters with no labels\n",
    "# KMeans, Gaussian Mixture, Power Iteration Clustering (IPC), Bisecting KMeans clustering, streaming KMeans clustering\n",
    "\n",
    "# KMeans\n",
    "RDD = sc.textFile(\"Data.csv\"). \\\n",
    "        map(lambda x: x.split(\",\")).\\\n",
    "        map(lambda x: [float(x[0]), float(x[1])])\n",
    "RDD.take(5)\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "model = KMeans.train(RDD, k=2, maxIterations=10)\n",
    "model.clusterCenters\n",
    "\n",
    "# evaluate the model by computing the error function\n",
    "from math import sqrt\n",
    "def error(point):\n",
    "    center = model.center[model.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSE = RDD.map(lambda point: error(point).reduce(lambda x, y: x + y))\n",
    "print(\"Within Set Sum of Square Error = \" + str(WSSSE))\n",
    "\n",
    "# Visualizing KMeans\n",
    "data_df = spark.createDataFrame(RDD, schema=[\"col1\", \"col2\"])\n",
    "data_df_pandas = spark.toPandas()\n",
    "\n",
    "cluster_center_pandas = pd.DataFrame(model.clusterCenters, columns=[\"col1\", \"col2\"])\n",
    "cluster_center_pandas.head()\n",
    "\n",
    "plt.scatter(data_df_pandas[\"col1\"], data_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_center_pandas[\"col1\"], cluster_center_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "\n",
    "# PRACTICE\n",
    "# Load the dataset into a RDD\n",
    "clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split('\\t'))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))\n",
    "\n",
    "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(16, WSSSE))\n",
    "\n",
    "# Train the model again with the best k \n",
    "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters\n",
    "\n",
    "# Convert rdd_split_int RDD into Spark DataFrame\n",
    "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame\n",
    "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
    "\n",
    "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(rdd_split_int_df_pandas, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# Create an overlaid scatter plot\n",
    "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
