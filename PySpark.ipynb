{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD\n",
    "# 3 methods\n",
    "# 1st: pass an existing object to SparkContext's parallelize method\n",
    "# 2nd: load data from an external hard drive (HDFS) or from Amazon s3 bucket or lines from a text\n",
    "# 3rd: from an existing RDD\n",
    "\n",
    "# 1st method: parallelized collection \n",
    "numRDD = sc.parallelize([1, 2, 3, 4])\n",
    "helloRDD = sc.parallelize('Hello World')\n",
    "type(helloRDD)\n",
    "\n",
    "# create RDD using the external datasets, textFile() method\n",
    "fileRDD = sc.textFile('README.md')\n",
    "type(fileRDD)\n",
    "\n",
    "\n",
    "# LOAD DATA INTO RDDs\n",
    "# understanding how Spark deals with partitions allows us to control parallelism\n",
    "# create an RDD using SparkContext's parallelize method with 6 partitions\n",
    "numRDD = sc.parallelize(range(10), minPartitions=6)\n",
    "\n",
    "# or we can use this method\n",
    "numRDD = sc.textFile('README.md', minPartitions=6)\n",
    "\n",
    "print('The number of partitions in numRDD is', numRDD.getNumPartitions())\n",
    "\n",
    "# RDD operations in PySpark\n",
    "# operations = transformation + actions\n",
    "# transformation creates RDD, action computes on RDDs\n",
    "# transformations follow Lazy evaluation, which enables RDDs to be fault tolerant\n",
    "\n",
    "# map() transformation\n",
    "# create RDD using SparkContext's parallelize method\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "# apply map function to each element\n",
    "RDD_map = RDD.map(lambda x: x * x)\n",
    "\n",
    "# filter transformation returns a new RDD with elements that pass the condition\n",
    "RDD = sc.parallelize([1, 2, 3, 4])\n",
    "RDD_filter = RDD.filter(lambda x: x > 2)\n",
    "\n",
    "# flatMap transformation returns many values for each element in the original RDD\n",
    "RDD = sc.parallelize(['Hello world', 'How are you'])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# union transformation returns the union of one RDD with another RDD\n",
    "inputRDD = sc.textFile(\"logs.txt\")\n",
    "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
    "warningsRDD = inputRDD.filter(lambda x: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are operations applied to RDD and return a value\n",
    "# basic RDD actions: collect(), take(N), first(), count()\n",
    "RDD_map.collect()\n",
    "RDD_map.take(2)\n",
    "RDD_map.first()\n",
    "RDD_flatmap.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with RDD key/value pairs\n",
    "# pair RDDs: special data structure\n",
    "# pair RDDs: key = identifier, value = data\n",
    "# 2 ways to create a pair RDD: from a list of key-value tuple or from a regular RDD\n",
    "# get the data into the key/value form\n",
    "\n",
    "# create a pair RDD from a list of key-value tuple\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "# create a pair RDD from regular RDDs\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\n",
    "\n",
    "# some transformations for pairRDDs: reduceByKey(), groupByKey(), sortByKey(), join()\n",
    "\n",
    "# practice reduceByKey() transformation\n",
    "regularRDD = sc.parallelize([\"Messi\", 23], [\"Ronaldo\", 34]\n",
    "                            [\"Neymar\", 22], [\"Messi\", 24])\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y: x + y)\n",
    "pairRDD_reducebykey.collect()\n",
    "\n",
    "# sorting of data: sortByKey() transformation\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\n",
    "\n",
    "# group values with the same key: groupByKey()\n",
    "airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "    \n",
    "# join() transformation to connect two pair RDDs based on their key\n",
    "RDD1 = sc.parallelize([(\"Messi\", 24), (\"Ronaldo\", 32)], (\"Neymar\", 24))\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 40), (\"Neymar\", 120), (\"Messi\", 50)])\n",
    "RDD1.join(RDD2).collect()\n",
    "\n",
    "# Practice\n",
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# ADVANCED RDD actions\n",
    "# reduce() action - avoid collect() because of the size\n",
    "x = [1, 3, 4, 6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)\n",
    "# saveAsTextFile() - each partition is saved separately as a file inside a directory\n",
    "RDD.saveAsTextFile(\"FileName\")\n",
    "# coalesce() - save RDD as a single text file\n",
    "RDD.coalesce(1).saveAsTextFile(\"FileName\")\n",
    "\n",
    "# RDD actions: countByKey(), collectAsMap()\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "for k, v in rdd.countByKey().items():\n",
    "    print(k, v)\n",
    "    \n",
    "# collectAsMap(): returns k-v pairs as dict\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "\n",
    "# Practice\n",
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")\n",
    "\n",
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
    "\n",
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{} has {} counts\". format(word[1], word[0]))\n",
    "\n",
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "# PySpark DATAFRAME\n",
    "# Spark high level API for working with structured data\n",
    "# PySpark SQL is a Spark library for structured data\n",
    "# PySpark SQL provides the structure of data and the computation being performed\n",
    "# PySpark SQL provides a programming abstraction called DataFrames\n",
    "# SparkSession does for DataFrames what the SparkContext does for RDDs\n",
    "# SparkSession creates DataFrames, registers DataFrames as tables, executes SQL over tables, cache tables\n",
    "# DataFrames can be created by spark in two ways: from existing RDDs - createDataFrame(), from various sources using\n",
    "# SparkSession's read method\n",
    "# Schema is a structure of data in DataFrame, it helps Spark optimize queries on the data more efficiently\n",
    "\n",
    "# CREATE a DataFrame from RDD\n",
    "iphones_RDD = sc.parallelize([(\"XR\", 2018, 5.65, 2.79, 6.24),\n",
    "                              (\"Xs\", 2018, 5.94, 2.98, 6.84)])\n",
    "names = [\"Model\", \"Year\", \"Height\", \"Width\", \"Weight\"]\n",
    "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\n",
    "type(iphones_df)\n",
    "\n",
    "# CREATE a DataFrame from a csv file\n",
    "df_csv = spark.read.csv(\"filename.csv\", header=True, inferSchema=True)\n",
    "# CREATE a DataFrame from a json file\n",
    "df_csv = spark.read.json(\"filename.json\", header=True, inferSchema=True)\n",
    "\n",
    "# Practice\n",
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))\n",
    "\n",
    "# INTERACTING WITH PYSPARK DATAFRAMES\n",
    "# DataFrames support transformations and actions\n",
    "# Common DataFrame transformations: select(), filter(), groupby(), orderby(), dropDuplicates(), withColumnRenamed()\n",
    "# Common DataFrame actions: printSchema(), head(), show(), count(), columns(), describe()\n",
    "\n",
    "# select and show operations\n",
    "# pass column names inside select()\n",
    "df_id_age = test.select('Age')\n",
    "# show is an action that prints the first 20 rows by default\n",
    "df_id_age.show(3)\n",
    "# filter() filters out rows based on a condition, pass the column name and the value of what we want to filter\n",
    "new_df_age21 = new_df.filter(new_df.Age > 21)\n",
    "new_df_age21.show(3)\n",
    "\n",
    "# groupby() groups a variable\n",
    "test_df_age_group = test_df.groupby('Age')\n",
    "test_df_age_group.count().show(3)\n",
    "\n",
    "# orderby() sorts the dataframe based on one or more columns\n",
    "test_df_age_group.count().orderBy('Age').show(3)\n",
    "\n",
    "# dropDuplicates() removes the duplicate rows of a DataFrame\n",
    "test_df_no_dup = test_df.select('User ID', 'Gender', 'Age').dropDuplicates()\n",
    "test_df_no_dup.count()\n",
    "\n",
    "# withColumnRenamed() renames a column in the DataFrame\n",
    "test_df_sex = test_df.withColumnRenamed('Gender', 'Sex')\n",
    "\n",
    "# printSchema() prints the types of columns in DataFrames\n",
    "test_df.printSchema()\n",
    "\n",
    "# columns() prints the names of columns in DataFrame\n",
    "test_df.columns()\n",
    "\n",
    "# describe() summarizes the statistics of numerical columns in DataFrame\n",
    "test_df.describe().show()\n",
    "\n",
    "# PRACTICE\n",
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
    "\n",
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))\n",
    "\n",
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))\n",
    "\n",
    "\n",
    "# INTERACT WITH PySpark SQL using SQL Query\n",
    "# PySpark can interact with SparkSQL through DataFrame API and SQL Queries\n",
    "# SparkSession sql() executes SQL query\n",
    "# sql() takes a SQL statement as an argument and returns the results as DataFrame\n",
    "# SQL Queries cannot be run directly against DataFrames\n",
    "# To issue SQL Queries against an existing DataFrame, use createOrReplaceTempView() to build a temporary table\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT field1, field 2 FROM table1\")\n",
    "df2.collect()\n",
    "\n",
    "# SQL query to extract data\n",
    "test_df.createOrReplaceTempView(\"test_table\")\n",
    "query = '''SELECT Product_ID FROM test_table'''\n",
    "test_product_df = spark.sql(query)\n",
    "test_product_df.show(5)\n",
    "\n",
    "# PRACTICE\n",
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)\n",
    "\n",
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))\n",
    "\n",
    "# DATA VISUALIZATION with PySpark using DataFrames\n",
    "# 3 methods to plot graphs using PySpark DataFrames\n",
    "# 1st: pyspark_dist_explore\n",
    "# 2nd: toPandas()\n",
    "# 3rd: HandySpark\n",
    "\n",
    "# pyspark_dist_explore: hist(), distplot(), pandas_histogram()\n",
    "# draw a histogram\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "test_df_age = test_df.select('Age')\n",
    "hist(test_df_age, bins=20, color=\"red\")\n",
    "\n",
    "# convert PySpark DataFrames into Pandas DataFrames\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "test_df_sample_pandas = test_df_sample.toPandas()\n",
    "test_df_sample_pandas.hist('Age')\n",
    "\n",
    "# Pandas DataFrames vs PySpark DataFrames\n",
    "# Pandas = 1 server memory, PySpark = multiple servers\n",
    "# Pandas = mutable, PySpark = immutable\n",
    "# Pandas APIs support more operations\n",
    "\n",
    "# HandySpark method\n",
    "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "hdf = test_df.toHandy()\n",
    "hdf.cols['Age'].hist()\n",
    "\n",
    "# PRACTICE\n",
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()\n",
    "\n",
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))\n",
    "\n",
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT * FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
